AWSTemplateFormatVersion: "2010-09-09"
Description: Adobe Analytics Data Lake Base

Parameters: 

  AdobeAnalyticsDataFeedS3BucketName:
    Type: String

  AdobeAnalyticsDataLakeS3BucketName:
    Type: String

  AdobeAnalyticsGlueJobScriptsS3BucketName:
    Type: String

  GlueDataLakeDatabaseName:
    Type: String
    Default: adobe-analytics-data-lake

  AdobeAnalyticsGlueJobName:
    Type: String
    Default: adobe-analytics-feed-to-lake

  RedshiftSpectrumRoleName:
    Type: String
    Default: redshift-spectrum-adobe-analytics

  SNSTopic:
    Type: String
    Description: SNSTopic to send Cloudwatch alarm notifications to
    Default: ''

Conditions:

  SNSTopicProvided: !Not [!Equals [!Ref SNSTopic, '']]

Resources:

  # Glue Job Script/s
  AdobeAnalyticsGlueJobScriptsS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref AdobeAnalyticsGlueJobScriptsS3BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # Transformed data into Parquet format for OOM improvements in efficiency
  AdobeAnalyticsDataLakeS3Bucket:
    DeletionPolicy: Retain
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption: 
        ServerSideEncryptionConfiguration: 
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      BucketName: !Ref AdobeAnalyticsDataLakeS3BucketName
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # Create an AWS Glue database to hold table definitions
  AdobeAnalyticsGlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Ref GlueDataLakeDatabaseName
        Description: Database to hold tables for Adobe Analytics Data Lake

  # IAM role for the crawler
  HitDataCrawlerIAMRole:
     Type: AWS::IAM::Role
     Properties: 
       ManagedPolicyArns:
         - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
       AssumeRolePolicyDocument:
         Version: 2012-10-17
         Statement:
           - Effect: Allow
             Principal:
               Service:
                 - glue.amazonaws.com
             Action:
               - sts:AssumeRole
       Policies:
       - PolicyName: adobe-analytics-data-lake-crawler
         PolicyDocument:
           Version: 2012-10-17
           Statement:
           -
             Effect: Allow
             Action:
               - s3:ListBucket
               - s3:GetObject
             Resource:
               - !GetAtt AdobeAnalyticsDataLakeS3Bucket.Arn
               - !Sub '${AdobeAnalyticsDataLakeS3Bucket.Arn}/hit-data/*'

  HitDataCrawler:
     DependsOn:
       - HitDataCrawlerIAMRole
     Type: AWS::Glue::Crawler
     Properties:
       Name: adobe-analytics-data-lake-hit-data-crawler
       Role: !GetAtt HitDataCrawlerIAMRole.Arn
       Description: AWS Glue crawler that makes partitioned Adobe Analytics hit data available to Athena & Redshift Spectrum
       # The glue job runs at 4:30PM UTC (see that definition for reasons why) so we'll give it an hour (which is twice as long
       # as it usually takes) and then crawl to see updated partitions.
       Schedule:
         ScheduleExpression: "cron(30 17 * * ? *)"
       DatabaseName: !Ref AdobeAnalyticsGlueDatabase
       Targets:
         S3Targets:
           - Path: !Sub 's3://${AdobeAnalyticsDataLakeS3BucketName}/hit-data'
       Configuration: '{
         "Version": 1.0,
         "CrawlerOutput": {
           "Partitions": {
             "AddOrUpdateBehavior": "InheritFromTable"
           }
         }
       }'
       SchemaChangePolicy:
         DeleteBehavior: "DELETE_FROM_DATABASE"
         UpdateBehavior: "UPDATE_IN_DATABASE"

  # IAM role for the conversion job, S3 TSV to S3 Parquet
  AdobeTSVToParquetGlueJobIAMRole:
    Type: AWS::IAM::Role
    Properties: 
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
      - PolicyName: AdobeAnalyticsS3ProcessingAccess
        PolicyDocument:
          Version: 2012-10-17
          Statement:
          -
            # Glue script itself
            Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
            Resource:
              - !GetAtt AdobeAnalyticsGlueJobScriptsS3Bucket.Arn
              - !Sub '${AdobeAnalyticsGlueJobScriptsS3Bucket.Arn}/hit-data.py'
          -
            # Glue job temp dir
            Effect: Allow
            Action:
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
              - s3:GetObjectVersion
              - s3:DeleteObjectVersion
              - s3:PutObjectAcl
              - s3:GetObjectAcl
            Resource:
              - !Sub '${AdobeAnalyticsGlueJobScriptsS3Bucket.Arn}/hit-data-temp/*'
          -
            # Source data feed bucket
            Effect: Allow
            Action:
              # Needs more than read because it moves things to processing
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
            Resource:
              - !Sub "arn:aws:s3:::${AdobeAnalyticsDataFeedS3BucketName}"
              - !Sub "arn:aws:s3:::${AdobeAnalyticsDataFeedS3BucketName}/*"
          -
            # Target data lake bucket
            Effect: Allow
            Action:
              # Needs more than just put because it makes temporary objects, deletes temporary objects, renames objects, etc.
              - s3:GetObject
              - s3:PutObject
              - s3:DeleteObject
              - s3:GetObjectVersion
              - s3:DeleteObjectVersion
              - s3:PutObjectAcl
              - s3:GetObjectAcl
            Resource:
              - !Sub '${AdobeAnalyticsDataLakeS3Bucket.Arn}/*'

  AdobeTSVToParquetGlueJob:
    Type: AWS::Glue::Job   
    DependsOn:
      AdobeTSVToParquetGlueJobIAMRole
    Properties:
      Role: !Ref AdobeTSVToParquetGlueJobIAMRole
      Description: Converts TSV files from the Adobe Analytics Data Feed in one S3 bucket into the much more efficient Parquet format and stored in another bucket.  Also generates a partitioning column, and does comma-delimited to array conversion.
      Command:   
        Name: glueetl
        ScriptLocation: !Sub 's3://${AdobeAnalyticsGlueJobScriptsS3BucketName}/hit-data.py'
      DefaultArguments: {
        "--TempDir": !Sub "s3://${AdobeAnalyticsGlueJobScriptsS3BucketName}/hit-data-temp",
        "--enable-metrics": "",
        "--s3source": !Sub "s3://${AdobeAnalyticsDataFeedS3BucketName}",
        "--s3target": !Sub "s3://${AdobeAnalyticsDataLakeS3BucketName}/hit-data"
      }
      AllocatedCapacity: 10
      ExecutionProperty:   
        # This must only ever be one or bad stuff might happen
        MaxConcurrentRuns: 1  
      Name: !Ref AdobeAnalyticsGlueJobName

  AdobeTSVToParquetGlueJobScheduleTrigger:
    Type: AWS::Glue::Trigger   
    DependsOn:
      - AdobeTSVToParquetGlueJob
    Properties:
      Name: !Sub '${AdobeAnalyticsGlueJobName}-daily'
      Description: Trigger for TSV to Parquet conversion
      Type: SCHEDULED                                                        	   
      Actions:
        - JobName: !Ref AdobeTSVToParquetGlueJob                	  
      # Trigger daily to try to minimise parquet files.  Timing is tweaked like this: hourly files
      # arrive from Adobe at 16 to 20 mins past the hour.  So we want to run our daily cron at half past midnight,
      # which just helpfully minimises the number of parquet files for each partition.  Then we need to
      # shift that minus 8 hours to UTC which is 4:30PM.
      Schedule: cron(30 16 * * ? *)

  JobFailureRule:
    Condition: SNSTopicProvided
    Type: AWS::Events::Rule
    Properties:
      Description: Adobe Analytics Data Lake TSV to Parquet Job Failure
      EventPattern:
        source:
          - aws.glue
        detail-type:
          - Glue Job State Change
        detail:
          state:
            - FAILED
            - TIMEOUT
          jobName:
            - !Ref AdobeTSVToParquetGlueJob
      State: ENABLED
      Targets:
        - Arn: !Ref SNSTopic
          Id: "SNSTopic"

  RedshiftSpectrumRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Ref RedshiftSpectrumRoleName
      AssumeRolePolicyDocument:
        Statement:
          - Action: sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - redshift.amazonaws.com
        Version: "2008-10-17"
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonAthenaFullAccess
        - arn:aws:iam::aws:policy/AWSGlueConsoleFullAccess
      Policies:
        - PolicyDocument:
            Statement:
              - Action:
                  - s3:Get*
                  - s3:List*
                Effect: Allow
                Resource:
                  - !Sub "arn:aws:s3:::${AdobeAnalyticsDataFeedS3BucketName}"
                  - !Sub "arn:aws:s3:::${AdobeAnalyticsDataFeedS3BucketName}/*"
                  - !GetAtt AdobeAnalyticsDataLakeS3Bucket.Arn
                  - !Sub '${AdobeAnalyticsDataLakeS3Bucket.Arn}/*'
                  - !GetAtt AdobeAnalyticsGlueJobScriptsS3Bucket.Arn
                  - !Sub '${AdobeAnalyticsGlueJobScriptsS3Bucket.Arn}/*'
            Version: "2012-10-17"
          PolicyName: redshift-spectrum-adobe-analytics

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
            - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: "/"
      Policies:
      - PolicyName: DeleteTemporaryFiles
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          -
            Effect: Allow
            Action:
              - s3:List*
            Resource:
              - !GetAtt AdobeAnalyticsDataLakeS3Bucket.Arn
              - !Sub '${AdobeAnalyticsDataLakeS3Bucket.Arn}/*'
          -
            Effect: Allow
            Action:
              - s3:DeleteObject
            Resource:
              - !Sub '${AdobeAnalyticsDataLakeS3Bucket.Arn}/*_$folder$'

  LambdaSchedule:
    Type: "AWS::Events::Rule"
    Properties:
      ScheduleExpression: rate(1 day)
      State: ENABLED
      Targets:
        - Arn: !Sub ${LambdaFunction.Arn}
          Id: LambdaSchedule

  LambdaSchedulePermission:
    Type: "AWS::Lambda::Permission"
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Sub ${LambdaFunction.Arn}
      Principal: 'events.amazonaws.com'
      SourceArn: !Sub ${LambdaSchedule.Arn}

  LambdaFunction:
    Type: "AWS::Lambda::Function"
    Properties:
      Code:
        ZipFile: |
          import boto3
          from datetime import datetime, timezone, timedelta
          import os

          s3 = boto3.client('s3')
          def lambda_handler(event, context):
              kwargs = {'Bucket': os.environ['LAKE_BUCKET']}
              is_truncated = True
              deletables = set()
              while is_truncated:
                  response = s3.list_objects_v2(**kwargs)
                  if response['IsTruncated']:
                      kwargs['ContinuationToken'] = response['NextContinuationToken']
                  else:
                      is_truncated = False
                  if 'Contents' in response.keys():
                      for obj in response['Contents']:
                          if obj['Key'].endswith('_$folder$') \
                              and obj['LastModified'] < datetime.now(timezone.utc) - timedelta(hours=1):
                              deletables.add(obj['Key'])
              if len(deletables) > 0:
                  delete = {
                      'Objects': [{'Key': obj} for obj in deletables]
                  }
                  s3.delete_objects(Bucket=os.environ['LAKE_BUCKET'], Delete=delete)
              return
      Handler: index.lambda_handler
      Environment:
        Variables:
          LAKE_BUCKET: !Ref AdobeAnalyticsDataLakeS3BucketName
      MemorySize: 128
      Timeout: 900
      Role: !Sub ${LambdaExecutionRole.Arn}
      Runtime: python3.7

Outputs:
  RedshiftSpectrumRoleARN:
    Description: The ARN of the Redshift Spectrum role
    Value: !GetAtt RedshiftSpectrumRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-RedshiftSpectrumRoleARN'
